{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c05ee576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "SEED = 22\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device('cpu')\n",
    "print('Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29101d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 60000 Test size: 10000\n"
     ]
    }
   ],
   "source": [
    "# 2) Data loader: MNIST padded to 32x32 (paper used 32x32 inputs)\n",
    "data_dir = '../data'\n",
    "batch_size = 64  # adjust for quicker runs\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(2),  # 28x28 -> 32x32\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "train_dataset = datasets.MNIST(root=data_dir, train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=data_dir, train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "print('Train size:', len(train_dataset), 'Test size:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f0c9201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Building blocks from the paper\n",
    "class ScaledTanh(nn.Module):\n",
    "    \"\"\"Scaled Tanh: f(x) = 1.7159 * tanh(2/3 * x)\n",
    "    as used in the original paper.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a = 1.7159\n",
    "        self.b = 2.0/3.0\n",
    "    def forward(self, x):\n",
    "        return self.a * torch.tanh(self.b * x)\n",
    "\n",
    "class Subsampling(nn.Module):\n",
    "    \"\"\"Average subsampling (2x2) followed by a learnable scale and bias, then a sigmoid.\n",
    "    This implements S2 and S4 as in the original paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        # Scale and bias per channel (broadcast over HxW)\n",
    "        self.scale = nn.Parameter(torch.ones(1, in_channels, 1, 1))\n",
    "        self.bias = nn.Parameter(torch.zeros(1, in_channels, 1, 1))\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        # Paper then applies sigmoid to (w * avg + b)\n",
    "        return torch.sigmoid(x * self.scale + self.bias)\n",
    "\n",
    "class C3Layer(nn.Module):\n",
    "    \"\"\"Sparse connected convolutional layer (C3) using the connection table from the paper.\n",
    "    Produces 16 output feature maps from 6 input maps.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Connection table from paper (16 lists connecting to 6 S2 maps)\n",
    "        self.connections = [\n",
    "            [0,1,2], [1,2,3], [2,3,4], [3,4,5], [0,4,5], [0,1,5],\n",
    "            [0,1,2,3], [1,2,3,4], [2,3,4,5], [0,3,4,5], [0,1,4,5], [0,1,2,5],\n",
    "            [0,1,3,4], [1,2,4,5], [0,2,3,5], [0,1,2,3,4,5]\n",
    "        ]\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(len(conn), 1, kernel_size=5) for conn in self.connections])\n",
    "    def forward(self, x):\n",
    "        outs = []\n",
    "        for i, conn in enumerate(self.connections):\n",
    "            sl = x[:, conn, :, :]  # select input channels\n",
    "            out = self.convs[i](sl)\n",
    "            outs.append(out)\n",
    "        # concatenate to get 16 output maps\n",
    "        return torch.cat(outs, dim=1)\n",
    "\n",
    "class RBFOutput(nn.Module):\n",
    "    \"\"\"RBF-style output layer: compute squared Euclidean distance to class prototype vectors.\n",
    "    For practical training we return negative distances as logits so CrossEntropyLoss can be used.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        # prototypes shape: (num_classes, input_dim)\n",
    "        self.prototypes = nn.Parameter(torch.randn(num_classes, input_dim) * 0.01)\n",
    "    def forward(self, x):\n",
    "        # x: (batch, input_dim)\n",
    "        # compute squared distances to prototypes: (batch, num_classes)\n",
    "        # dist^2 = ||x||^2 + ||p||^2 - 2 x.p\n",
    "        x_norm = (x**2).sum(dim=1, keepdim=True)  # (batch,1)\n",
    "        p_norm = (self.prototypes**2).sum(dim=1).unsqueeze(0)  # (1, num_classes)\n",
    "        cross = x @ self.prototypes.t()  # (batch, num_classes)\n",
    "        d2 = x_norm + p_norm - 2*cross\n",
    "        # return negative distances as logits (larger => closer => larger logit)\n",
    "        return -d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc3eec74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "# 4) LeNet-5 model wiring as described in paper\n",
    "class LeNet5Paper(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # C1: 1 -> 6 maps, 5x5\n",
    "        self.c1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.tanh = ScaledTanh()\n",
    "        # S2: subsampling\n",
    "        self.s2 = Subsampling(6)\n",
    "        # C3: sparse -> 16 maps\n",
    "        self.c3 = C3Layer()\n",
    "        # S4\n",
    "        self.s4 = Subsampling(16)\n",
    "        # C5: 16 -> 120 via 5x5 conv -> produces 120x1x1 if input is 5x5\n",
    "        self.c5 = nn.Conv2d(16, 120, kernel_size=5)\n",
    "        self.f6 = nn.Linear(120, 84)\n",
    "        # RBF output: input dim 84 -> 10 classes\n",
    "        self.rbf = RBFOutput(input_dim=84, num_classes=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B,1,32,32)\n",
    "        x = self.c1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.s2(x)  # 6x14x14\n",
    "        x = self.c3(x)  # 16x10x10\n",
    "        x = self.tanh(x)\n",
    "        x = self.s4(x)  # 16x5x5\n",
    "        x = self.c5(x)  # 120x1x1\n",
    "        x = self.tanh(x)\n",
    "        x = x.view(x.size(0), -1)  # (B, 120)\n",
    "        x = self.f6(x)  # (B, 84)\n",
    "        x = self.tanh(x)\n",
    "        logits = self.rbf(x)  # (B, 10) negative distances\n",
    "        return logits\n",
    "\n",
    "model = LeNet5Paper().to(DEVICE)\n",
    "dummy = torch.randn(2,1,32,32)\n",
    "out = model(dummy)\n",
    "print('Output shape:', out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5122100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Training utilities and paper-like schedule\n",
    "def paper_lr_schedule(epoch):\n",
    "    # example schedule approximating the paper's annealing behavior\n",
    "    if epoch < 2:\n",
    "        return 0.0005\n",
    "    elif epoch < 5:\n",
    "        return 0.0002\n",
    "    elif epoch < 8:\n",
    "        return 0.0001\n",
    "    elif epoch < 12:\n",
    "        return 0.00005\n",
    "    else:\n",
    "        return 0.00001\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs=10, initial_lr=0.01, use_paper_schedule=True, save_dir='./logs/lenet_paper'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=initial_lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    history = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        if use_paper_schedule:\n",
    "            lr = paper_lr_schedule(epoch)\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = lr\n",
    "        else:\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, preds = logits.max(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100.0 * correct / total\n",
    "\n",
    "        # validation\n",
    "        val_loss, val_acc = evaluate(model, test_loader, criterion)\n",
    "\n",
    "        history.append({'epoch': epoch+1, 'lr': lr, 'train_loss': train_loss, 'train_acc': train_acc, 'val_loss': val_loss, 'val_acc': val_acc})\n",
    "        print(f'Epoch {epoch+1}/{epochs} | lr={lr:.6f} | train_loss={train_loss:.4f} acc={train_acc:.2f}% | val_loss={val_loss:.4f} val_acc={val_acc:.2f}%')\n",
    "\n",
    "    # Save final model and history\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, 'lenet_paper.pth'))\n",
    "    pd.DataFrame(history).to_csv(os.path.join(save_dir, 'history.csv'), index=False)\n",
    "    return history\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, preds = logits.max(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return running_loss / len(loader), 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffbbbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | lr=0.000500 | train_loss=2.3019 acc=10.62% | val_loss=2.3012 val_acc=11.35%\n",
      "Epoch 2/20 | lr=0.000500 | train_loss=2.3011 acc=11.24% | val_loss=2.3008 val_acc=11.35%\n"
     ]
    }
   ],
   "source": [
    "# Quick smoke training (2 epochs)\n",
    "smoke_model = LeNet5Paper().to(DEVICE)\n",
    "history = train_model(smoke_model, train_loader, test_loader, epochs=20, initial_lr=0.01, use_paper_schedule=True, save_dir='./logs/lenet_paper')\n",
    "pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a755dcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Evaluation & visualizations\n",
    "# Load history if saved\n",
    "hist_path = './logs/lenet_paper/history.csv'\n",
    "if os.path.exists(hist_path):\n",
    "    df = pd.read_csv(hist_path)\n",
    "else:\n",
    "    df = pd.DataFrame(history)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(df['epoch'], df['train_loss'], label='train_loss')\n",
    "plt.plot(df['epoch'], df['val_loss'], label='val_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(df['epoch'], df['train_acc'], label='train_acc')\n",
    "plt.plot(df['epoch'], df['val_acc'], label='val_acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy (%)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrix on test set\n",
    "# load saved model if exists\n",
    "model = LeNet5Paper().to(DEVICE)\n",
    "model_path = './logs/lenet_paper/lenet_paper.pth'\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    print('Loaded trained model for analysis')\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        logits = model(imgs.to(DEVICE))\n",
    "        _, preds = logits.max(1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(8,6))\n",
    "import seaborn as sns\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f506d48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Visualize first-layer filters (C1)\n",
    "w = model.c1.weight.data.clone().cpu()  # (6,1,5,5)\n",
    "fig, axes = plt.subplots(1,6, figsize=(12,3))\n",
    "for i in range(6):\n",
    "    ax = axes[i]\n",
    "    ax.imshow(w[i,0], cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('C1 filters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ae6f25-697c-4e31-a7bf-2485bb067a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
